% !TEX encoding = UTF-8 Unicode 
% !TEX root = praca.tex

\chapter{Related work}

Table \ref{tab:related_work} contains the selection of literature directly related to the topic of this thesis. Analysis of the related work should provide additional insights into the scope of carried out research, and potentially reveal obsolescence or shortfalls present in the previous research.

\begin{longtblr}[
    caption = {Related work (Source: Own work)},
    label = {tab:related_work},
]{ colspec = { |c|X| }, hlines} 
    \textbf{Paper}&\textbf{Key takeaways}\\
    \cite{comparison_perf_looks_flutter_native}&M. Ollson compares a mobile app developed using Kotlin, Swift and Flutter. The measures considered are: code size, development time and CPU usage. Additionally, a survey is performed among 39 programmers to assess the native look and feel of the Flutter application. (2020)\\
    \cite{eval_rn_flutter}&E. Hjort performs an evaluation of two cross-platform frameworks: Flutter and React Native aiming to select the ideal solution for a selected company. Based on that company's requirements, the technical and business criteria are selected with different weights mapped to them. (2020)\\
    \cite{cross_platform_development_study_rn_flutter}&A. E. Fentaw compares a COVID-19 tracking application implemented with both Flutter and React Native. The criterion considered is the app performance in the form of CPU, GPU and memory consumption. (2020)\\
    \cite{comparison_technologies_multiplatform}&J. Crha performs a comparative analysis of different approaches to cross-platform development: cross-compiled frameworks and Responsive Web Applications. Flutter and React Native are evaluated based on app performance, primarily startup time, CPU and memory usage, frames per second (FPS) and app bundle size. (2021)\\
    \cite{denko_comp_hybrid}&B. Denko, S. Pecnik and I. Fister Jr. perform a four-phase evaluation of different multi-platform solutions: Ionic, React Native, NativeScript and Flutter compared to native approach. A single target platform is considered (Android). The main criteria considered are app size and installation time, CPU and RAM usage and algorithm execution time. (2021)\\
    \cite{comp_analysis_hybrid_frameworks}&M. Singh and G. Shobha compare multiple solutions to cross-platform development: React Native, Ionic, Flutter and Xamarin. The criteria considered are exclusively coming from the development perspective, e.g. IDEs, platform support, code reusability and testing process. (2021)\\
    \cite{bialkowski_eval_flutter}&D. Białkowski and J. Smołka perform a comparison of time performance of Android apps implemented natively (Java) and with a cross-platform framework (Flutter). There are three research scenarios proposed applied per each application. (2022)\\
    \cite{kocki_comp_hybrid_ios}&M. Kocki, M. Urban and P. Kopniak compare the native and cross-platform approaches to iOS development in the form of Swift and Ionic. The research focuses on compilation time, database read and write time and data sorting execution time. (2022)\\
    \cite{approach_to_assess_performance_case_study}&D. Mota and R. Martinho introduce a stable approach to performance assessment of mobile apps developed with cross-platform frameworks. They propose a set of metrics (CPU usage, RAM usage, execution time, FPS), as well as evaluation features and result normalization method. Thereafter, the proposed solution is applied to compare Flutter and React Native. (2021)\\
    \cite{willocx_quantative_perf}&M. Willocx, J. Vossaert and V. Naessens propose an approach to mobile app performance assessment on the example of Xamarin and PhoneGap. Based on the results, some guidelines for framework selection are suggested. The performance metrics selected are: launch time, pause and resume time, time to open page, memory consumption, CPU usage and disk space. (2015)\\
    \cite{hort_survey_perf_optimization}&M. Hort, M. Kechagia, F. Sarro and M. Harman describe a variety of mobile application performance optimization techniques inspired by the literature from 2008-2020. The metrics considered are responsiveness, launch time, memory usage and energy consumption. (2022)\\
    \cite{survey_taxonomy_cross_platform}&A. Biørn-Hansen and T-M. Grønli and G. Ghinea provide a detailed overview of approaches to cross-platform development, as well as correct misconceptions found in other literature. For a group of concepts (User Experience, Software Platform Features, Performance and Hardware, Security) the state-of-research is described and suggestions for future work are proposed. (2018)\\
    \cite{rieger_eval_cp}&C. Rieger and T. A. Majchrzak propose a detailed framework consisting of 33 criteria divided into 4 perspectives: Infrastructure, Development, App and Usage. The purpose of such framework is to be used for the complete assessment of a cross-platform solution. In this case, completeness means considering both technical and business aspects. (2019)\\
    \cite{cross_platform_ux}&E. Angulo and X. Ferre perform a case study to determine the capability of cross-platform frameworks to provide a high-level user experience and usability compared to native solutions. (2014)\\
\end{longtblr}

Numerous research papers exist on the topic of mobile application performance among cross-platform frameworks. However, their results tend to differ from each other, especially considering work conducted across a broad timeframe. The underlying issue is that existing cross-platform frameworks are constantly changing in order to improve their usability, as well as the quality of applications developed with them. Moreover, completely new solutions are being introduced. It seems the research in this scope should be carried out on a sporadic basis so that the up-to-date findings can be applied in decision-making.

\clearpage
